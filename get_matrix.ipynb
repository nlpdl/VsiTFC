{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 计算相似度\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "def get_alignment(de_raw_sentence: List[str], embedding_type):\n",
    "    de_alignment = []\n",
    "    id = 0\n",
    "    de_raw_tokens = de_raw_sentence.split()\n",
    "    for word in de_raw_tokens:\n",
    "        de_alignment.extend([id for _ in range(len(get_tokenized_tokens(word, embedding_type)))])\n",
    "        id += 1\n",
    "#     print(de_alignment, len(de_alignment))\n",
    "    assert id == len(de_raw_tokens), (id, len(de_raw_tokens))\n",
    "    de_alignment_dict = Counter(de_alignment)\n",
    "    return de_alignment, de_alignment_dict\n",
    "\n",
    "\n",
    "def cosine_distance(a, b):\n",
    "    if a.shape != b.shape:\n",
    "        raise RuntimeError(\"array {} shape not match {}\".format(a.shape, b.shape))\n",
    "    if a.ndim==1:\n",
    "        a_norm = np.linalg.norm(a)\n",
    "        b_norm = np.linalg.norm(b)\n",
    "    elif a.ndim==2:\n",
    "        a_norm = np.linalg.norm(a, axis=1, keepdims=True)\n",
    "        b_norm = np.linalg.norm(b, axis=1, keepdims=True)\n",
    "    else:\n",
    "        raise RuntimeError(\"array dimensions {} not right\".format(a.ndim))\n",
    "    similiarity = np.dot(a, b.T)/(a_norm * b_norm)\n",
    "    dist = 1. - similiarity\n",
    "    return dist\n",
    "\n",
    "\n",
    "\n",
    "def get_aligned_embeddings(de_alignment, de_alignment_dict, de_sentence_embed):\n",
    "    id, de_emb_average, alignment_de_embeds = 0, 0., []\n",
    "    \n",
    "    for i, de_emb in enumerate(de_sentence_embed):\n",
    "        ### apply alignment\n",
    "        if de_alignment[i] == id:\n",
    "            de_emb_average += de_emb\n",
    "        else:\n",
    "            de_emb_average /= de_alignment_dict[id]\n",
    "            alignment_de_embeds.append(de_emb_average)\n",
    "            id += 1\n",
    "            de_emb_average = de_emb\n",
    "    de_emb_average  /= de_alignment_dict[id]\n",
    "    alignment_de_embeds.append(de_emb_average)\n",
    "    assert len(alignment_de_embeds) == max(de_alignment)+1, (len(alignment_de_embeds), max(de_alignment)+1)\n",
    "    return alignment_de_embeds\n",
    "                        \n",
    "def compute_word2words_similarity(keys, query, sim_type=1,\n",
    "                                  norm_similarity=False, k=2, sparse_top_k=False):\n",
    "    similarity = []\n",
    "    for i, key in enumerate(keys):\n",
    "        if sim_type == 1: similarity.append(1-cosine_distance(query.cpu(), key.cpu()))\n",
    "        elif sim_type == 2: similarity.append(np.exp(-np.linalg.norm(query-key, ord=2, axis=-1)))\n",
    "        else: e\n",
    "    \n",
    "    if sparse_top_k:\n",
    "        top_k_similarity = sorted(similarity, reverse=True)[k-1]\n",
    "        similarity = [i if i >= top_k_similarity else 0 for i in similarity]\n",
    "    if norm_similarity:\n",
    "        similarity = np.array(similarity) / np.array(similarity).sum()\n",
    "        similarity = similarity.tolist()\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取分词工具\n",
    "from bert_tokenizer import BertTokenizer\n",
    "tokenizer = BertTokenizer.load_from_vocab_file('/home/sxy/Projects/CP/OVC-MMT/data/MY_MULT30K/distil_cased_based/vocab.txt')\n",
    "tokenizer._cased = True\n",
    "multi_bert_tokenizer = BertTokenizer.load_from_vocab_file('/home/sxy/Projects/CP/OVC-MMT/data/MY_MULT30K/multi_cased_L-12_H-768_A-12/vocab.txt')\n",
    "multi_bert_tokenizer._cased = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mbert\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_type = 4 # 1 for static word embedding and 2 for dynamic token embedding\n",
    "\n",
    "### bert model\n",
    "## distiluse bert \n",
    "embedder = SentenceTransformer('distiluse-base-multilingual-cased')#'bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 词向量工具\n",
    "import numpy as np\n",
    "\n",
    "def get_tokenized_tokens(string, embedding_type):\n",
    "    if embedding_type == 1: en_temp = [string]\n",
    "    elif embedding_type == 2: en_temp = tokenizer.tokenize(string)\n",
    "    else: en_temp = multi_bert_tokenizer.tokenize(string)\n",
    "    return en_temp\n",
    "\n",
    "\n",
    "\n",
    "def get_embedding(string : str, embedding_type=1):\n",
    "    if embedding_type == 1:   # seq_len * embed_dim\n",
    "        return [model[w] for w in tokenizer.tokenize(string)]\n",
    "    elif embedding_type == 2: # seq_len * embed_dim\n",
    "        return [(embedder.encode(w, output_value = 'token_embeddings')[0]).mean(axis=0) for w in string.split()][:len(string.split())]\n",
    "        return [(embedder.encode(w, output_value = 'token_embeddings')[0][1:-1]).mean(axis=0) for w in string.split()]\n",
    "    elif embedding_type == 3: # seq_len * embed_dim\n",
    "        return embedder.encode(string, output_value = 'token_embeddings')[1:-1]\n",
    "    elif embedding_type == 4: # seq_len * embed_dim\n",
    "        return embedder.encode(string, output_value = 'token_embeddings').mean(axis=0)\n",
    "    elif embedding_type == 5: # seq_len * embed_dim\n",
    "        embs = []\n",
    "        for w in string.split():\n",
    "            emb, sub_ws = 0., multi_bert_tokenizer.tokenize(w)\n",
    "            print(sub_ws)\n",
    "#             emb = (embedder.encode(sub_ws[0], output_value = 'token_embeddings')).mean(axis=0)\n",
    "            for sub_w in sub_ws: emb += (embedder.encode(sub_w, output_value = 'token_embeddings')).mean(axis=0)\n",
    "            embs.append(emb/float(len(sub_ws)))\n",
    "#             embs.append(emb)\n",
    "        return embs\n",
    "    elif embedding_type == 6:\n",
    "        embs = []\n",
    "        for w in string.split():\n",
    "            emb = (embedder.encode(w, output_value = 'token_embeddings')).mean(axis=0)\n",
    "            embs.append(emb)\n",
    "        return embs\n",
    "    elif embedding_type == 7:\n",
    "        embs = []\n",
    "        for w in string.split():\n",
    "            emb, sub_ws = 0., multi_bert_tokenizer.tokenize(w)\n",
    "            print(sub_ws)\n",
    "            for sub_w in sub_ws:\n",
    "                try:\n",
    "                    emb += (model[sub_w])\n",
    "                except:\n",
    "                    emb += np.full(768,0)\n",
    "                    continue\n",
    "            embs.append(emb/float(len(sub_ws)))\n",
    "        return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 获取obj信息\n",
    "import numpy as np\n",
    "spilit = 'train'\n",
    "npzfile = '/home/sxy/Projects/CP/OVC-MMT/img_out/flickr30k-images/'\n",
    "items_obj = []\n",
    "items_att = []\n",
    "with open('/home/sxy/Projects/CP/OVC-MMT/data/MY_MULT30K/%s.txt' % spilit, 'r') as f:\n",
    "    for i in f.readlines():\n",
    "        file = i.split('.')[0]\n",
    "        it = np.load(npzfile+file+'.npz',allow_pickle=True)\n",
    "        temp_obj = it['info'].tolist()['objects_id'].tolist()\n",
    "        temp_att = it['info'].tolist()['attrs_id'].tolist()\n",
    "        items_obj.append(temp_obj)\n",
    "        items_att.append(temp_att)\n",
    "attributes = []\n",
    "classes = [] \n",
    "with open('/home/sxy/Projects/CP/OVC-MMT/scripts/raw_data/vcr/attributes_vocab.txt', 'r') as f1:\n",
    "    for att in f1.readlines():\n",
    "        attributes.append(att.split(',')[0].lower().strip())\n",
    "    \n",
    "with open('/home/sxy/Projects/CP/OVC-MMT/scripts/raw_data/vcr/objects_vocab.txt', 'r') as f2: \n",
    "    for object in f2.readlines():\n",
    "        classes.append(object.split(',')[0].lower().strip())\n",
    "\n",
    "\n",
    "\n",
    "def get_object_list(temp_obj_list,temp_att_list):\n",
    "    all_list = []\n",
    "    \n",
    "    for i,j in zip(temp_att_list,temp_obj_list):\n",
    "        all_list.append(attributes[i]+' '+classes[j])\n",
    "    return all_list\n",
    "\n",
    "\n",
    "obj_all_list = []#存放每一句话的所有信息\n",
    "for obj,att in zip(items_obj,items_att):\n",
    "        line = get_object_list(obj,att)\n",
    "        obj_all_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# sentence_path = '/home/sxy/Projects/CP/multi30k-wmt18/data/task1/tok/test_2017_flickr.lc.norm.tok.en'\n",
    "sentence_path = '/home/sxy/Projects/CP/multi30k-wmt18/data/task1/tok/train.lc.norm.tok.en'\n",
    "sentence_list = []\n",
    "with open(sentence_path, 'r',) as r:\n",
    "    for line in r.readlines():\n",
    "        sentence_list.append(line.split())\n",
    "print(len(max(sentence_list,key = len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matrix = np.lib.format.open_memmap(\n",
    "        '/home/sxy/Projects/CP/work/data-bin/muti30k_en_de_bpe10k/' + \"train_matrix0.8.npy\", mode=\"w+\",\n",
    "        dtype=np.float16, shape=(len(sentence_list),50,10))\n",
    "import torch\n",
    "test = torch.zeros(50,10)#初始化一个50*10的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "threshold = 0.8\n",
    "embedding_type, align_similairty = 3, False\n",
    "if embedding_type == 3: align_similairty = True\n",
    "index = 0\n",
    "for sentence,obj in zip(sentence_list,obj_all_list):#遍历每一行\n",
    "#     print(sentence)\n",
    "    for sentence_index in range(len(sentence)):#遍历每一个词\n",
    "#         print(sentence[sentence_index])\n",
    "        now_sentence_embd = get_embedding(sentence[sentence_index], embedding_type)#获取当前词bert向量\n",
    "        now_sentence_token = tokenizer.tokenize(sentence[sentence_index])#获取当前词分词\n",
    "        for obj_index in range(len(obj)):\n",
    "            now_obj_embd = get_embedding(obj[obj_index], embedding_type)#获取当前词bert向量,一个特征两个词\n",
    "            obj_alignment, obj_alignment_dict = get_alignment(obj[obj_index], embedding_type)#分词列表\n",
    "#             print(obj[obj_index])\n",
    "            keys = now_obj_embd\n",
    "            map_dict = {}\n",
    "            for txt, query in zip(now_sentence_token, now_sentence_embd):\n",
    "                similarity = compute_word2words_similarity(keys, query, sim_type=1)#获取一个相似度列表\n",
    "                if align_similairty:\n",
    "                    new_similarity = []\n",
    "                    for id in range(max(obj_alignment)+1):\n",
    "                        similarity_temp = [similarity[i] for i, a in enumerate(obj_alignment) if a == id]\n",
    "    #                     print(similarity_temp)\n",
    "                        if len(similarity_temp) == 0:\n",
    "                            new_similarity.append([])\n",
    "                        else:\n",
    "                            new_similarity.append(max(similarity_temp))\n",
    "                    similarity = new_similarity\n",
    "                old_imilarity = 0\n",
    "#                 print('--------------------')\n",
    "#                 print(obj[obj_index])\n",
    "                for a, b in zip(obj[obj_index].split(), similarity):\n",
    "#                     print(sentence[sentence_index])\n",
    "                    if b >= threshold and b > old_imilarity:#如果相似度对应的单词阈值大于0.7，且比其他的大，更新对应词表\n",
    "                        matrix[index][sentence_index][obj_index] = 1\n",
    "                        break\n",
    "    index = index + 1\n",
    "matrix.flush()\n",
    "                        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 获取obj信息\n",
    "import numpy as np\n",
    "spilit = 'val'\n",
    "npzfile = '/home/sxy/Projects/CP/OVC-MMT/img_out/flickr30k-images/'\n",
    "items_obj = []\n",
    "items_att = []\n",
    "with open('/home/sxy/Projects/CP/OVC-MMT/data/MY_MULT30K/%s.txt' % spilit, 'r') as f:\n",
    "    for i in f.readlines():\n",
    "        file = i.split('.')[0]\n",
    "        it = np.load(npzfile+file+'.npz',allow_pickle=True)\n",
    "        temp_obj = it['info'].tolist()['objects_id'].tolist()\n",
    "        temp_att = it['info'].tolist()['attrs_id'].tolist()\n",
    "        items_obj.append(temp_obj)\n",
    "        items_att.append(temp_att)\n",
    "attributes = []\n",
    "classes = [] \n",
    "with open('/home/sxy/Projects/CP/OVC-MMT/scripts/raw_data/vcr/attributes_vocab.txt', 'r') as f1:\n",
    "    for att in f1.readlines():\n",
    "        attributes.append(att.split(',')[0].lower().strip())\n",
    "    \n",
    "with open('/home/sxy/Projects/CP/OVC-MMT/scripts/raw_data/vcr/objects_vocab.txt', 'r') as f2: \n",
    "    for object in f2.readlines():\n",
    "        classes.append(object.split(',')[0].lower().strip())\n",
    "\n",
    "\n",
    "\n",
    "def get_object_list(temp_obj_list,temp_att_list):\n",
    "    all_list = []\n",
    "    \n",
    "    for i,j in zip(temp_att_list,temp_obj_list):\n",
    "        all_list.append(attributes[i]+' '+classes[j])\n",
    "    return all_list\n",
    "\n",
    "\n",
    "obj_all_list = []#存放每一句话的所有信息\n",
    "for obj,att in zip(items_obj,items_att):\n",
    "        line = get_object_list(obj,att)\n",
    "        obj_all_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# sentence_path = '/home/sxy/Projects/CP/multi30k-wmt18/data/task1/tok/test_2017_flickr.lc.norm.tok.en'\n",
    "sentence_path = '/home/sxy/Projects/CP/multi30k-wmt18/data/task1/tok/val.lc.norm.tok.fr'\n",
    "sentence_list = []\n",
    "with open(sentence_path, 'r',) as r:\n",
    "    for line in r.readlines():\n",
    "        sentence_list.append(line.split())\n",
    "print(len(max(sentence_list,key = len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_o_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "threshold = 0.4\n",
    "embedding_type, align_similairty = 3, False\n",
    "if embedding_type == 3: align_similairty = True\n",
    "index = 0\n",
    "for sentence,obj in zip(sentence_list,obj_all_list):#遍历每一行\n",
    "    sentence_list_dict = {}\n",
    "#     print(sentence)\n",
    "    for sentence_index in range(len(sentence)):#遍历每一个词\n",
    "#         print(sentence[sentence_index])\n",
    "        now_sentence_embd = get_embedding(sentence[sentence_index], embedding_type)#获取当前词bert向量\n",
    "        now_sentence_token = tokenizer.tokenize(sentence[sentence_index])#获取当前词分词\n",
    "        for obj_index in range(len(obj)):\n",
    "            now_obj_embd = get_embedding(obj[obj_index], embedding_type)#获取当前词bert向量,一个特征两个词\n",
    "            obj_alignment, obj_alignment_dict = get_alignment(obj[obj_index], embedding_type)#分词列表\n",
    "#             print(obj[obj_index])\n",
    "            keys = now_obj_embd\n",
    "            map_dict = {}\n",
    "            for txt, query in zip(now_sentence_token, now_sentence_embd):\n",
    "                similarity = compute_word2words_similarity(keys, query, sim_type=1)#获取一个相似度列表\n",
    "                if align_similairty:\n",
    "                    new_similarity = []\n",
    "                    for id in range(max(obj_alignment)+1):\n",
    "                        similarity_temp = [similarity[i] for i, a in enumerate(obj_alignment) if a == id]\n",
    "    #                     print(similarity_temp)\n",
    "                        if len(similarity_temp) == 0:\n",
    "                            new_similarity.append([])\n",
    "                        else:\n",
    "                            new_similarity.append(max(similarity_temp))\n",
    "                    similarity = new_similarity\n",
    "                old_imilarity = 0\n",
    "#                 print('--------------------')\n",
    "#                 print(obj[obj_index])\n",
    "                for a, b in zip(obj[obj_index].split(), similarity):\n",
    "#                     print(sentence[sentence_index])\n",
    "                    if b >= threshold and b > old_imilarity:#如果相似度对应的单词阈值大于0.7，且比其他的大，更新对应词表\n",
    "                        if sentence_index in sentence_list_dict:\n",
    "                            sentence_list_dict[sentence_index].append(obj_index)\n",
    "                        else:\n",
    "                            sentence_list_dict[sentence_index] = [copy.deepcopy(obj_index)]\n",
    "    fr_o_list.append(sentence_list_dict)\n",
    "out = '/home/sxy/Projects/CP/OVC-MMT/data/MY_MULT30K/fr/valid_fr_o0.4.txt'\n",
    "with open(out,'w') as w:\n",
    "    for i in fr_o_list:\n",
    "        item = json.dumps(i,ensure_ascii=False)\n",
    "        w.write(item)\n",
    "        w.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [1, 2, 3, 4, 5, 6, 7]}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {}\n",
    "a[1] = [1,2,3,4,5,6]\n",
    "a[1].append(7)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairseq",
   "language": "python",
   "name": "fairseq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
